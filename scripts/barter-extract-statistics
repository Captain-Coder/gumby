#!/usr/bin/env python
import sys
from time import mktime
import re
from os import listdir
import os
from sys import argv, exit, maxint
from Tribler.dispersy.tool.ldecoder import parse, parselast

# Global variable (yes, not the brightest thing)
measured_record_count = {} # dictionary with the total record count per node at time measure_interval_stop
record_count = {} # same as before but at experiment end
traffic_count = {} # dictionary with the total traffic count per node at time measure_interval_stop
node_count = 0 

def get_nodes(peer_dir):
    global node_count
    pattern = re.compile('[0-9]{5}')
    for d in listdir(peer_dir):
        if pattern.match(d):
            node_nr = int(d)
            
            dispersy_exists = os.path.exists(os.path.join(peer_dir, d, 'output','dispersy.log'))
            if node_nr <= node_count and dispersy_exists:
                yield peer_dir + "/" + d

def get_title(node):
    if re.match(".*[0-9]{5}", node):
        return node[-5:]
    elif len(node)>7 and node[-7:] == "tracker":
        return "tracker"
    return "???"

def get_first_datetime(peers_directory):
    datetimes = []
    for node in get_nodes(peers_directory):
        try:
            _, _, _, kargs = parse(node + "/output/dispersy.log", ('joined-community', )).next()
            if 'starting_timestamp' in kargs:
                datetimes.append(kargs['starting_timestamp'])
        except:
            pass
    
    #Fallback to old method
    if not datetimes:
        print >> sys.stderr, "USING fallback for get_first_datetime"
        for node in get_nodes(peers_directory):
            _, time, _, _ = parse(node + "/output/dispersy.log").next()
            datetimes.append(time)
            
    return min(datetimes)

def get_communities(peers_directory):
    communities = set()
    for node in get_nodes(peers_directory):
        try:
            _, _, _, kargs = parselast(node + "/output/dispersy.log", ('statistics', ), chars = 2048).next()
            if 'candidates' in kargs:
#                print >> sys.stderr, node, kargs['candidates']
                for community, nrcandidates in kargs['candidates']:
                    communities.add(community)
        except:
            pass
    
    communities = list(communities)
    communities.sort()
    return communities

def generate(peers_directory, messagestoplot):
    messagestoplot = [message for message in messagestoplot if message]
    first = maxint
    start = int(get_first_datetime(peers_directory))
    last = 0
    communities = get_communities(peers_directory)

    node_list = [x for x in get_nodes(peers_directory)]
    def node_cmp(a, b):
        a = int(a[-5:])
        b = int(b[-5:])
        return cmp(a, b)
    node_list.sort(cmp=node_cmp)

    dispersy_msg_distribution = {}
    dispersy_dropped_msg_distribution = {}
    dispersy_bootstrap_distribution = {}
    dispersy_statistics = {}
    dispersy_in_out = {}
    nr_connections = []

    for node in node_list:
        nodename = node[-5:]
        
        traffic_start = traffic_stop = None
        
        c_created_record = 0
        c_created_records = {}
        c_received_record = 0
        c_received_records = {}
        c_dropped_record = 0
        c_communities = {}
        cur_incomming_connections = max_incomming_connections = 0

        h_received_record = open(node + "/output/received-record.txt", "w+")
        print >> h_received_record, "# timestamp timeoffset num-records"
        print >> h_received_record, "0 0 0"

        h_created_record = open(node + "/output/created-record.txt", "w+")
        print >> h_created_record, "# timestamp timeoffset num-records"
        print >> h_created_record, "0 0 0"

        h_total_record = open(node + "/output/total-record.txt", "w+")
        print >> h_total_record, "# timestamp timeoffset num-records"
        print >> h_total_record, "0 0 0"
        
        h_total_connections = open(node + "/output/total-connections.txt", "w+")
        print >> h_total_connections, "# timestamp timeoffset (num-connections +)"
        print >> h_total_connections, "#", " ".join(communities) 
        print >> h_total_connections, "0 0" + (" 0"*len(communities))
        
        h_statistics = open(node + "/output/scenario-statistics.txt", "w+")
        print >> h_statistics, "# timestamp timeoffset key value"

        h_drop = open(node + "/output/drop.txt", "w+")
        print >> h_drop, "# timestamp timeoffset num-drops"
        print >> h_drop, "0 0 0"

        h_stat = open(node + "/output/stat.txt", "w+")
        print >> h_stat, "# timestamp timeoffset total-send total-received"
        print >> h_stat, "0 0 0 0"

        print  nodename, " ",
        record_count[nodename] = {}
        dispersy_in_out[nodename] = [0,0,0,0]
        for lineno, time, message, kargs in parse(node + "/output/dispersy.log"):
            time = int(time)
            timeoffset = time - start
            first = min(first, timeoffset)
            
            writeTotal = False
            
            if message == "statistics":
                if "total_down" in kargs:
                    dispersy_in_out[nodename][0] = kargs["total_down"]
                if "total_up" in kargs:
                    dispersy_in_out[nodename][1] = kargs["total_up"]
                if "total_send" in kargs:
                    dispersy_in_out[nodename][2] = kargs["total_send"]
                if "received_count" in kargs:
                    dispersy_in_out[nodename][3] = kargs["received_count"]
                    
                if "total_down" in kargs or "total_up" in kargs:
                    print >> h_stat, str(time), str(timeoffset), str(dispersy_in_out[nodename][1]) , str(dispersy_in_out[nodename][0])
                    
                    if timeoffset > 0:
                        traffic_stop = dispersy_in_out[nodename][0] + dispersy_in_out[nodename][1]
                        if not traffic_start:
                            traffic_start = traffic_stop
                
                if "drop_count" in kargs:
                    c_dropped_record = kargs["drop_count"]
                    print >> h_drop, str(time), str(timeoffset), str(c_dropped_record)
                    
                if 'candidates' in kargs:
                    for community, nrcandidates in kargs['candidates']:
                        c_communities[community] = nrcandidates
                    
                    max_incomming_connections = max(max(c_communities.values()), max_incomming_connections)
                    print >> h_total_connections, str(time), str(timeoffset),
                    for community in communities:
                        print >> h_total_connections, c_communities.get(community, 0),
                    print >> h_total_connections, ''
            
            if message == "statistics-successful-messages":
                for key, value in kargs.iteritems():
                    dispersy_msg_distribution[key] = max((value, node), dispersy_msg_distribution.get(key, (0, node)))
                    
                    if not messagestoplot or key in messagestoplot:
                        c_received_records[key] = value
                        writeTotal = True
                        
                if writeTotal:
                    c_received_record = sum(c_received_records.itervalues())
                    print >> h_received_record, str(time), str(timeoffset), str(c_received_record)
                    
            if message == "statistics-created-messages":
                for key, value in kargs.iteritems():
                    if not messagestoplot or key in messagestoplot:
                        c_created_records[key] = value
                        writeTotal = True
                        
                if writeTotal:
                    c_created_record  = sum(c_created_records.itervalues())
                    print >> h_created_record, str(time), str(timeoffset), str(c_created_record)
                    
            if message == "scenario-statistics":
                for key, value in kargs.iteritems():
                    print >> h_statistics, str(time), str(timeoffset), key, value

            if message == "statistics-dropped-messages":
                for key, value in kargs.iteritems():
                    dispersy_dropped_msg_distribution[key] = max((value, node), dispersy_dropped_msg_distribution.get(key, (0, node)))

            if message == "statistics-bootstrap-candidates":
                for key, value in kargs.iteritems():
                    if key not in dispersy_bootstrap_distribution:
                        dispersy_bootstrap_distribution[key] = {}
                    dispersy_bootstrap_distribution[key][node] = value
            
            if writeTotal:
                print >> h_total_record, str(time), str(timeoffset), str(c_received_record + c_created_record)
                
                record_count[nodename][timeoffset] = [c_received_record, c_created_record, c_received_record + c_created_record, 0.0]
                measured_record_count[nodename] = record_count[nodename][timeoffset]

        print ""
        if not traffic_start:
            print "!!!! No statistics collected"
        else:
            traffic_count[nodename] = [traffic_start, traffic_stop, traffic_stop - traffic_start]
            
        print >> h_received_record, str(time), str(timeoffset), str(c_received_record)
        h_received_record.close()
        
        print >> h_created_record, str(time), str(timeoffset), str(c_created_record)
        h_created_record.close()
        
        print >> h_total_record, str(time), str(timeoffset), str(c_received_record + c_created_record)
        h_total_record.close()
        
        print >> h_drop, str(time), str(timeoffset), str(c_dropped_record)
        h_drop.close()
        
        print >> h_total_connections, str(time), str(timeoffset),
        for community in communities:
            print >> h_total_connections, c_communities.get(community, 0),
        print >> h_total_connections, ''
        h_total_connections.close()
        
        h_statistics.close()
        h_stat.close()
        
        last = max(last, timeoffset)        
        nr_connections.append((max_incomming_connections, nodename))
        nr_connections.sort(reverse = True)
        nr_connections = nr_connections[:10]
    
    f = open(peers_directory + "/output/dispersy_incomming_connections.txt", 'w')
    for nr, node in nr_connections:
        print >> f, node, nr
    f.close() 

    # returns the first and the last last timetime
    experiment_length = start+last - start
    merge_records(peers_directory, "/output/total-record.txt", '/output/sum_total_records.txt', 2)
    merge_records(peers_directory, "/output/stat.txt", '/output/send.txt', 2, '/output/send_diff.txt')
    merge_records(peers_directory, "/output/stat.txt", '/output/received.txt', 3, '/output/received_diff.txt')
    merge_records(peers_directory, "/output/drop.txt", '/output/dropped.txt', 2, '/output/dropped_diff.txt')
    
    sum_at_experiment_end = sum_created_records(peers_directory)
    
    print "# experiment took", experiment_length, "seconds"
    print "# number of records at experiment end (%d): %d records" %(experiment_length, sum_at_experiment_end)
    h_first_last = open(peers_directory + "/output/first_last.txt", "w")
    print >> h_first_last, "%d %d" %(first, last)
    h_first_last.close()

    h_dispersy_msg_distribution = open(peers_directory + "/output/dispersy-msg-distribution.txt", "w+")
    print >> h_dispersy_msg_distribution, "# msg_name count"
    for msg, count in dispersy_msg_distribution.iteritems():
        print >> h_dispersy_msg_distribution, "%s %d %s" %(msg, count[0], count[1])
    h_dispersy_msg_distribution.close()
    
    h_dispersy_bootstrap_distribution = open(peers_directory + "/output/dispersy-bootstrap-distribution.txt", "w+")
    print >> h_dispersy_bootstrap_distribution, "# sock_addr count"

    bootstrap_keys = dispersy_bootstrap_distribution.keys()
    bootstrap_keys.sort(cmp = lambda a,b: cmp(sum(dispersy_bootstrap_distribution[a].values()), sum(dispersy_bootstrap_distribution[b].values())), reverse = True)
    for sock_addr in bootstrap_keys:
        nodes = dispersy_bootstrap_distribution[sock_addr]
        times = sum(nodes.values())
        print >> h_dispersy_bootstrap_distribution, "%s %d" %(str(sock_addr), times)
    h_dispersy_bootstrap_distribution.close()
    
    h_dispersy_dropped_msg_distribution = open(peers_directory + "/output/dispersy-dropped-msg-distribution.txt", "w+")
    print >> h_dispersy_dropped_msg_distribution, "# msg_name count"
    for msg, count in dispersy_dropped_msg_distribution.iteritems():
        print >> h_dispersy_dropped_msg_distribution, "%s %d %s" %(msg, count[0], count[1])
    h_dispersy_dropped_msg_distribution.close()

    h_traffic_count = open(peers_directory + "/output/traffic_count_per_peer.txt", "w+")
    print >> h_traffic_count, "# peer-id traffic_start traffic_stop traffic_total"
    sum_traffic = 0
    for node in traffic_count.keys():
        v = traffic_count[node]
        sum_traffic += v[2]
        print >> h_traffic_count, "%s %d %d %d" %(node, v[0], v[1], v[2])
    h_traffic_count.close()

    h_record_count = open(peers_directory + "/output/records_per_peer.txt", "w+")
    print >> h_record_count, "# peer-id records_received records_created records_total coverage"
    sum_coverage = 0.0
    for node in measured_record_count.keys():
        v = measured_record_count[node]
        v[3] = float(v[2])*100/sum_at_experiment_end
        sum_coverage += v[3]
        print >> h_record_count, "%s %d %d %d %f" %(node, v[0], v[1], v[2], v[3])
    h_record_count.close()
    l = len(measured_record_count)
    if l != 0:
        average_coverage = sum_coverage/l
    else:
        average_coverage = 0.0

    # count how many records were there in the system at each point in time
    records_in_time = {}
    for node in record_count.keys():
        update_count = len(record_count[node])
        update_times = sorted(record_count[node].keys())
        
        for i in range(update_count):
            extra = record_count[node][update_times[i]][2] # total number of records is in the list on the 3rd position
            if extra < sum_at_experiment_end: continue
            if i == update_count - 1:
                last_time = experiment_length + 1
            else:
                last_time = update_times[i+1]
                
            for time in range(update_times[i], last_time):
                records_in_time[time] = records_in_time.get(time, 0) + 1

    fn = peers_directory + "/output/peers_with_all_records_in_time.txt"
    h = open(fn, "w+")
    print >> h, "# time record_count"
    times = records_in_time.keys()
    times.sort()
    for time in times:
        print >> h, "%d %d"%(time, records_in_time[time])
    h.close()
    
    parse_statistics(peers_directory)

    bytes_per_second_experiment = float(sum_traffic)/len(traffic_count)/experiment_length
    
    bytes_in = sum(total_down for total_down, _,_,_ in dispersy_in_out.values())
    bytes_out = sum(total_up for _,total_up,_,_ in dispersy_in_out.values())
    bytes_lost = bytes_out - bytes_in
    bytes_lost_ratio = bytes_lost/float(bytes_out)
    
    in_requests = sum(received_count for _,_,_,received_count in dispersy_in_out.values())
    out_requests = sum(total_send for _,_,total_send,_ in dispersy_in_out.values())

    fn_measure_info = peers_directory + "/output/measure_info.txt"
    h_measure_info = open(fn_measure_info, "w+")
    print >> h_measure_info, "Experiment length: %d" %(experiment_length)
    print >> h_measure_info, "Total records at experiment end: %d" %(sum_at_experiment_end)
    print >> h_measure_info, "Average coverage at experiment end: %f" %(average_coverage)
    print >> h_measure_info, "Bytes per second experiment: %f" %(bytes_per_second_experiment)
    print >> h_measure_info, "Bytes out: %d" %(bytes_out)
    print >> h_measure_info, "Bytes in: %d" %(bytes_in)
    print >> h_measure_info, "Bytes lost: %d" %(bytes_lost)
    print >> h_measure_info, "Bytes lost_ratio: %.2f" %(bytes_lost_ratio)
    print >> h_measure_info, "In requests: %d" %(in_requests)
    print >> h_measure_info, "Out requests: %d" %(out_requests)
    h_measure_info.close()
    return first, last

def sum_created_records(peers_directory):
    """ I create a file under 'peers/' called 'sum_created_records.txt'
    with all the available records in the system per timestamp
    """
    sum_records = {}
    for node in get_nodes(peers_directory):
        fn_created_record = node + "/output/created-record.txt"
        h_created_record = open(fn_created_record)
        prev_created = 0
        for line in h_created_record:
            if line[0] == "#": continue
            _, time, records = line.split()
            time = int(time)
            created = int(records)
            diff = created - prev_created
            if diff:
                sum_records[time] = sum_records.get(time, 0) + diff
                prev_created = created
        h_created_record.close()

    sum_at_measure_time = -1
    determined_sum = False

    fp = open(peers_directory + '/output/sum_created_records.txt', 'wb')
    sumr = 0
    for time in sorted(sum_records.iterkeys()):
        sumr += sum_records[time]
        print >> fp, "%s %s" % (time, sumr)
        
    return sumr
    
def merge_records(peers_directory, inputfile, outputfile, columnindex, diffoutputfile = None):
    all_nodes = []
    
    sum_records = {}
    for node in get_nodes(peers_directory):
        _,nodename = os.path.split(node)
        all_nodes.append(nodename)
        
        fn_records = node + inputfile
        h_records = open(fn_records)
        for line in h_records:
            if line[0] == "#": continue
            
            parts = line.split()
            if len(parts) > columnindex:
                time = int(parts[1])
                record = float(parts[columnindex])
                if record == 0: continue # skip over lines that do not contain bartercast updates
                sum_records.setdefault(time, {})[nodename] = record
        h_records.close()

    all_nodes.sort()

    fp = open(peers_directory + outputfile, 'wb')
    fp2 = open(peers_directory + diffoutputfile, 'wb') if diffoutputfile else None
    
    print >> fp, 'time', ' '.join(all_nodes)
    if fp2:
        print >> fp2, 'time', ' '.join(all_nodes)
    
    prev_records = {}
    for time in sorted(sum_records.iterkeys()):
        print >> fp, time,
        if fp2:
            print >> fp2, time,
        
        nodes = sum_records[time]
        for node in all_nodes:
            value = nodes.get(node, prev_records.get(node, 0))
            print >> fp, value,
            
            if fp2:
                diff = value - prev_records.get(node, 0)
                print >> fp2, diff,
            
            prev_records[node] = value
        print >> fp, ''
        if fp2:
            print >> fp2, ''
        
    fp.close()
    if fp2:
        fp2.close()
    
def parse_statistics(peers_directory):
    """ I created a file under 'peers/' called 'sum_scenario_statiscitcs.txt'
    with all the available records in the system per timestamp
    """
    sum_records = {}
    timeoffset_dict = {}
    nodes = set()
    for node in get_nodes(peers_directory):
        nodes.add(node)
        
        h_scenario_statistics = open(node + "/output/scenario-statistics.txt")
        for line in h_scenario_statistics:
            if line[0] == "#": continue
            parts = line.split()

            time = int(parts[0])
            timeoffset = int(parts[1])
            key = parts[2]
            value = float(parts[3])
            
            timeoffset_dict[time] = timeoffset
                
            if key not in sum_records:
                sum_records[key] = {}
            if time not in sum_records[key]:
                sum_records[key][time] = {}
            sum_records[key][time][node] = value
            
        h_scenario_statistics.close()

    nodes = list(nodes)
    nodes.sort()
    
    fp = open(peers_directory + '/output/sum_statictics.txt', 'w')
    keys = sum_records.keys()
    keys.sort()
        
    print >> fp, "#", "time", "timeoffset", "values"
    print >> fp, " ".join(keys)
    
    if keys:
        timestamps = sum_records[keys[0]].keys()
        timestamps.sort()
        
        prev_values = {}
        for time in timestamps:
            print >> fp, time, timeoffset_dict[time],
            for key in keys:
                sum = 0
                for node in nodes:
                    if key not in prev_values:
                        prev_values[key] = {}
                        
                    prev_value = prev_values[key].get(node, 0)
                    value = sum_records[key][time].get(node, prev_value)
                    prev_values[key][node] = value
                    
                    sum += value
                print >> fp, sum,
            print >> fp, ''
    fp.close()

def main(peers_directory, messagestoplot):
    global node_count
    peer_count = open(peers_directory+'/peer.count', 'r')
    node_count = int(peer_count.readline())
    peer_count.close()
    
    generate(peers_directory, messagestoplot.split(','))

if __name__ == "__main__":
    if len(argv) != 3:
        print "Usage: %s <peers-directory> <messagestoplot>" %(argv[0])
        import sys
        print >> sys.stderr, argv
        
        exit(1)

    main(argv[1], argv[2])
